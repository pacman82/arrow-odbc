[package]
name = "arrow-odbc"
version = "19.1.1"
authors = ["Markus Klein"]
edition = "2024"
license = "MIT"
repository = "https://github.com/pacman82/arrow-odbc"
documentation = "https://docs.rs/arrow-odbc/"

# A short blurb about the package. This is not rendered in any format when
# uploaded to crates.io (aka this is not markdown).
description = "Read/Write Apache Arrow arrays from/to ODBC data sources."

# This is a list of up to five keywords that describe this crate. Keywords
# are searchable on crates.io, and you may choose any words that would
# help someone find this crate.
keywords = ["odbc", "database", "sql", "arrow"]

# This is a list of up to five categories where this crate would fit.
# Categories are a fixed list available at crates.io/category_slugs, and
# they must match exactly.
categories = ["database"]

# This points to a file under the package root (relative to this `Cargo.toml`).
# The contents of this file are stored and indexed in the registry.
# crates.io will render this file and place the result on the crate's page.
readme = "Readme.md"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
arrow = { version = ">= 29, < 57", default-features = false, features = [
    "chrono-tz",
] }
# We use chrono for date conversions.
chrono = "0.4.39"
log = "0.4.27"
thiserror = "2.0.12"

odbc-api = ">= 17, < 18"
atoi = "2.0.0"


[dev-dependencies]
anyhow = "1.0.97"
# Used to share one ODBC environment in tests
lazy_static = "1.5.0"
# Function name macro is used to ensure unique table names in test
stdext = "0.3.3"
float_eq = "1.0.1"

[features]

# On linux we assume use of a UTF-8 locale. So we set the narrow features implying that for queries,
# connection strings and error messages the driver and driver manager supply utf8-strings. This
# might also be slightly faster since no transcoding between UTF-8 and UTF-16 is required.
# Overall speed always depends on the driver, but it is reasonable to assume this is faster, more
# importantly, UTF-8 is likely to be more battled tested on these platforms, while UTF-16 is "only"
# required by the standard.

# On windows we can not assume the default locale to be UTF-8, so we compile odbc-api with default
# features implying the use of UTF-16 for queries, connection strings and error messages. This
# should work on any system. However if you would like to use the narrow UTF-8 function calls on
# windows systems you can set this feature flag.
narrow = ["odbc-api/narrow"]

# On linux we assume use of a UTF-8 locale. So we set the narrow features implying that for queries,
# connection strings and error messages the driver and driver manager supply utf8-strings. This
# might also be slightly faster since no transcoding between UTF-8 and UTF-16 is required.
# Overall speed always depends on the driver, but it is reasonable to assume this is faster, more
# importantly, UTF-8 is likely to be more battled tested on these platforms, while UTF-16 is "only"
# required by the standard. However, if you are e.g. faced with a driver which does not use UTF-8,
# but only ascii, or want to use the wide functions calls for any other reason on a non-windows
# system you can set the `wide` feature flag to overwrite this behavior.
wide = ["odbc-api/wide"]
